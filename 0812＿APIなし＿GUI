import sys
import os
from bs4 import BeautifulSoup
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
import time
import tkinter as tk
from tkinter import messagebox

def start_scraping():
    # GUIからの入力を取得
    search_term = search_term_entry.get()
    max_iterations = int(max_iterations_entry.get())

    # SeleniumのWebDriverを設定
    driver_path = r'C:\Users\yukik\Desktop\chromedriver.exe'  # あなたのchromedriver.exeのパスを指定してください
    service = Service(driver_path)
    driver = webdriver.Chrome(service=service)

    # 初期ページを開く
    url = f'https://www.ncbi.nlm.nih.gov/books/?term={search_term}'  # 検索キーワードをURLに組み込む
    driver.get(url)

    titles = []
    links = []

    current_iteration = 0

    while current_iteration < max_iterations:
        # ページの内容をBeautifulSoupで解析
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        booklists = soup.find_all('p', class_='title')  # p.title を全て取得

        # 書籍タイトルとリンクを取得
        for booklist in booklists:
            a_tag = booklist.find('a', href=True)
            if a_tag:
                titles.append(a_tag.get_text().strip())  # タイトルを取得してリストに追加
                links.append('https://www.ncbi.nlm.nih.gov' + a_tag['href'])  # フルリンクを作成してリストに追加

        try:
            # "Next" ボタンが表示されるまでスクロール
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)  # スクロール後の読み込みを待つ

            # "Next" ボタンがクリック可能になるまで待つ
            next_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.CLASS_NAME, "active.page_link.next"))
            )
            next_button.click()
            current_iteration += 1
        except Exception as e:
            # "Next" ボタンが見つからない場合はループを終了
            messagebox.showerror("エラー", f"次のページに移動できませんでした: {e}")
            break

    # ドライバーを閉じる
    driver.quit()

    # タイトルとリンクの長さを一致させる
    max_length = max(len(titles), len(links))
    titles.extend([''] * (max_length - len(titles)))
    links.extend([''] * (max_length - len(links)))

    # 解析したデータを辞書に保存
    data = {
        'Title': titles,
        'Link': links
    }

    # データフレームを作成
    df = pd.DataFrame(data)

    # 出力ファイルパスをユーザーのドキュメントフォルダに設定
    output_directory = os.path.join(os.path.expanduser('~'), 'Documents')
    output_file_path = os.path.join(output_directory, 'ncbi_result_info.xlsx')

    # データフレームをExcelファイルに書き出す
    df.to_excel(output_file_path, index=False, engine='openpyxl')

    # 完了メッセージを表示
    messagebox.showinfo("完了", f"Excelファイル '{output_file_path}' に保存されました！")

# Tkinterを使ってGUIを構築
root = tk.Tk()
root.title("NCBI 本棚のデータスクレイピング♪")

# ラベルと入力フィールド
tk.Label(root, text="検索ワードを入力してね！:").grid(row=0, column=0, padx=10, pady=5)
search_term_entry = tk.Entry(root, width=30)
search_term_entry.grid(row=0, column=1, padx=10, pady=5)

tk.Label(root, text="繰り返しの回数を指定してね！:").grid(row=1, column=0, padx=10, pady=5)
max_iterations_entry = tk.Entry(root, width=30)
max_iterations_entry.grid(row=1, column=1, padx=10, pady=5)

# 実行ボタン
tk.Button(root, text="実行", command=start_scraping).grid(row=2, columnspan=2, pady=10)

root.mainloop()
